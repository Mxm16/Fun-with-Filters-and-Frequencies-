<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Fun with Filters and Frequencies!</title>
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/modern-normalize/modern-normalize.css" />
  <style>
    :root {
      --accent: #2b6cb0;
      --bg: #f7fafc;
      --card: #ffffff;
      --muted: #6b7280;
    }

    body {
      font-family: Inter, system-ui, -apple-system, 'Segoe UI', Roboto, 'Helvetica Neue', Arial;
      background: var(--bg);
      color: #111;
      line-height: 1.6;
      margin: 0;
      padding: 0;
    }

    .container {
      max-width: 1100px;
      margin: 32px auto;
      padding: 20px;
    }

    header {
      display: flex;
      gap: 16px;
      align-items: center;
    }

    header h1 {
      margin: 0;
      font-size: 1.6rem;
    }

    nav {
      margin: 18px 0;
    }

    nav a {
      margin-right: 12px;
      color: var(--accent);
      text-decoration: none;
    }

    .grid {
      display: grid;
      grid-template-columns: 1fr;
      gap: 20px;
    }

    .card {
      background: var(--card);
      padding: 16px;
      border-radius: 12px;
      box-shadow: 0 6px 18px rgba(30,41,59,0.06);
    }

    h2 {
      margin-top: 0;
    }

    pre { background: #0b1220; color: #d1e8ff; padding: 12px; border-radius: 8px; overflow: visible; }


    code {
      font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, 'Roboto Mono', 'Courier New', monospace;
    }

    .img-row {
      display: flex;
      flex-wrap: wrap;
      gap: 10px;
      justify-content: flex-start;
    }

    .img-row img {
      max-width: 300px;
      width: auto;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 6px 18px rgba(2,6,23,0.06);
    }

    .note {
      background: #fff7ed;
      border-left: 4px solid #f6ad55;
      padding: 10px;
      border-radius: 6px;
      color: #92400e;
    }

    .code-box {
      background: #0b1220;
      border-radius: 8px;
      padding: 10px;
      margin: 10px 0;
      max-height: 360px;
      overflow: auto;
    }

    .code-box pre {
      margin: 0;
    }

    @media (max-width: 900px) {
      .container {
        padding: 12px;
      }
    }
  </style>
</head>
<body>
  <div class="container">
    <header>
      <div>
        <h1>Fun with Filters and Frequencies!</h1>
      </div>
    </header>

    <nav>
      <a href="#part1">Part 1: Filters</a>
      <a href="#part2">Part 2: Frequencies</a>
      <a href="#multires">Part 2.3/2.4: Multiresolution Blending</a>
    </nav>

    <div class="grid">
      <main>

        <!-- Part 1 -->
        <section id="part1" class="card">
          <h2>Part 1: Fun with Filters</h2>

<!-- 1.1 Convolutions from Scratch -->
<h3>1.1 Convolutions from Scratch</h3>
<p>
  Here I implement convolution with four nested loops, then optimize to two loops, 
  and finally compare with the built-in <code>scipy.signal.convolve2d</code>.
</p>

<div class="code-box">
<pre><code>import numpy as np
from scipy import signal
import matplotlib.pyplot as plt
from skimage import data

# Four-loop convolution
def conv4(image, kernel):
    Hi, Wi = image.shape
    Hk, Wk = kernel.shape
    pad_h, pad_w = Hk//2, Wk//2
    # Zero-padding: pad with 0 around the border
    padded = np.pad(image.astype(np.float64), ((pad_h, pad_h), (pad_w, pad_w)), 'constant')
    out = np.zeros((Hi, Wi), dtype=np.float64)
    kernel_flipped = np.flipud(np.fliplr(kernel))
    for i in range(Hi):
        for j in range(Wi):
            val = 0.0
            for m in range(Hk):
                for n in range(Wk):
                    val += padded[i+m, j+n] * kernel_flipped[m, n]
            out[i,j] = val
    return out

# Two-loop convolution
def conv2(image, kernel):
    Hi, Wi = image.shape
    Hk, Wk = kernel.shape
    pad_h, pad_w = Hk//2, Wk//2
    padded = np.pad(image.astype(np.float64), ((pad_h,pad_h),(pad_w,pad_w)), 'constant')
    out = np.zeros((Hi, Wi), dtype=np.float64)
    kernel_flipped = np.flipud(np.fliplr(kernel))
    for i in range(Hi):
        for j in range(Wi):
            out[i, j] = (padded[i:i+Hk, j:j+Wk] * kernel_flipped).sum()
    return out

# Example image
img = data.coins()
kernel = np.array([[1,0,-1],[1,0,-1],[1,0,-1]])

out4 = conv4(img, kernel)
out2 = conv2(img, kernel)
out_builtin = signal.convolve2d(img, kernel, mode='same')

plt.figure(figsize=(12,4))
plt.subplot(1,3,1)
plt.imshow(out4, cmap='gray')
plt.title("My Conv (4 loops)")

plt.subplot(1,3,2)
plt.imshow(out2, cmap='gray')
plt.title("My Conv (2 loops)")

plt.subplot(1,3,3)
plt.imshow(out_builtin, cmap='gray')
plt.title("scipy.signal.convolve2d")

plt.show()
</code></pre>
</div>

<div class="img-row">
  <img src="results/compare.png" alt="Comparison conv4 vs conv2 vs builtin" style="max-width: 90%;" />
</div>

<!-- Padding explanation -->
<p>
  <b>Padding Strategy:</b> To preserve the output size, we use <b>zero-padding</b>, i.e., padding the input image with 
  zeros around the border. When the convolution kernel extends beyond the edge of the image, the out-of-bound 
  pixels are treated as 0. This ensures that the output has the same dimensions as the original input.
</p>

<!-- Runtime comparison -->
<div class="code-box">
<pre><code>import time
t0 = time.perf_counter(); out4 = conv4(img, kernel); t4 = time.perf_counter()-t0
t0 = time.perf_counter(); out2 = conv2(img, kernel); t2 = time.perf_counter()-t0
t0 = time.perf_counter(); outb = signal.convolve2d(img, kernel, mode='same'); tb = time.perf_counter()-t0
print(f"conv4: {t4:.3f}s, conv2: {t2:.3f}s, builtin: {tb:.3f}s")
</code></pre>
</div>

<table>
  <tr><th>Method</th><th>Runtime (s)</th></tr>
  <tr><td>conv4 (4 loops)</td><td>0.373</td></tr>
  <tr><td>conv2 (2 loops)</td><td>0.303</td></tr>
  <tr><td>builtin (SciPy)</td><td>0.003</td></tr>
</table>

<p>
  <b>Analysis:</b> The built-in SciPy function is by far the fastest.
  The 2-loop version is faster than the 4-loop one, and reducing the number of loops and leveraging vectorized operations tends to improve speed. 
</p>

<p>
  Then, this is a picture of my cat Coconut (read as grayscale), and convolved with the box filter. 
  I also apply the finite difference operators Dx and Dy.
</p>

<div class="code-box">
<pre><code>import imageio.v2 as imageio
from skimage import color
import numpy as np
import matplotlib.pyplot as plt

myimg = imageio.imread('myself.jpg')
mygray = color.rgb2gray(myimg)

# 9x9 Box filter
box_filter = np.ones((9,9), dtype=np.float32)/81.0
blurred = conv2(mygray, box_filter)
plt.imshow(blurred, cmap='gray')
plt.title("Box Filter (9x9)")
plt.show()

# Finite difference operators
Dx = np.array([[1,-1]], dtype=np.float32)
Dy = np.array([[1],[ -1]], dtype=np.float32)

Ix = conv2(mygray, Dx)
Iy = conv2(mygray, Dy)

plt.figure(figsize=(10,4))
plt.subplot(1,2,1)
plt.imshow(Ix, cmap='gray')
plt.title("Dx (Horizontal Gradient)")

plt.subplot(1,2,2)
plt.imshow(Iy, cmap='gray')
plt.title("Dy (Vertical Gradient)")
plt.show()
</code></pre>
</div>

<div class="img-row">
  <img src="results/mycat.jpg" alt="Original" />
</div>
<div class="img-row">
  <img src="results/mycatgray.png" alt="Gray" />
  <img src="results/mycatdxdy.png" alt="DxDy" style="max-width: 100%;" />
</div>


          
          <!-- 1.2 -->
<h3>1.2 Finite Difference Operator (Dx, Dy)</h3>
<p>
  Here I apply finite difference operators <em>Dx</em> and <em>Dy</em> on 
  <em>cameraman</em> image to compute partial derivatives, then combine them to obtain
  the gradient magnitude. Finally, binarize the gradient magnitude with different thresholds
  to visualize edges.
</p>

<div class="img-row">
  <img src="results/cameramandxdy.png" alt="DxDy (horizontal and vertical derivatives)" style="max-width: 90%;" />
</div>

<p>
  Below are the gradient magnitude images binarized with several thresholds. A smaller threshold
  preserves more edges but also more noise; a larger threshold removes noise but may lose fine edges.
</p>

<div class="img-row">
  <img src="results/cameramanedges1.png" alt="Edges threshold=0.1" />
  <img src="results/cameramanedges15.png" alt="Edges threshold=0.15" />
  <img src="results/cameramanedges.png" alt="Edges threshold=0.2" />
  <img src="results/cameramanedges25.png" alt="Edges threshold=0.25" />
  <img src="results/cameramanedges3.png" alt="Edges threshold=0.3" />
</div>

<p>
  According to above, I chose <strong>threshold=0.2</strong> as a balance: it removes most small noise
  while keeping the main structures (such as the outline of the cameraman and tripod).
</p>

<div class="img-row">
  <img src="results/cameramanedges.png" alt="Final edge map (threshold=0.2)" />
</div>

          
<!-- 1.3 -->
<h3>1.3 Derivative of Gaussian (DoG) Filter</h3>

<p>
The previous results are quite noisy, with many spurious high-frequency responses appearing in flat regions.
Although edges can be detected, they appear rough and fragmented.</p>
<p>
To reduce the noise in the finite difference gradient, first smooth the image with a Gaussian filter <code>G</code>. 
Construct a 2D Gaussian filter by creating a 1D Gaussian using <code>cv2.getGaussianKernel()</code> and taking its outer product with its transpose.
</p>

<p>
Then compute the derivatives of the smoothed image using the finite difference operators <code>D_x</code> and <code>D_y</code>, obtaining <em>Ix</em> and <em>Iy</em>, and the gradient magnitude. Compared with the unsmoothed gradients, the smoothed gradients are less noisy and highlight the main edges more clearly.
</p>

<p>
Alternatively, construct DoG filters by convolving the Gaussian kernel <code>G</code> with <code>D_x</code> and <code>D_y</code> to produce <code>DoG_x</code> and <code>DoG_y</code>. Applying these DoG filters directly to the original image produces nearly identical results to smoothing first and then computing derivatives, as verified by the small numerical differences.
</p>

<div class="img-row">
  <img src="results/dog_combined.png" alt="Gaussian smoothing and DoG results" style="max-width: 90%;" />
</div>

<p>
Gaussian smoothing significantly reduces noise compared with the simple finite difference method, and the DoG filters combine smoothing and differentiation in a single convolution. This approach is more efficient and confirms that Gaussian smoothing and differentiation are commutative.
</p>

        <!-- Part 2 -->
        <section id="part2" class="card">
          <h2>Part 2: Fun with Frequencies</h2>

<h3>2.1 Image Sharpening (Unsharp Masking)</h3>
<p><code>sharpened = original + α * (original - blurred)</code></p>
<p>A Gaussian filter produces the blurred version. Subtracting it from the original extracts high frequencies, which are then scaled by α and added back to enhance edges and details. Below we show the comparison for the Taj Mahal image:</p>

          <div class="img-row">
            <img src="results/taj.png" alt="Taj" style="max-width: 90%;" />
          </div>

          <p>Another test on a picture of my cat Coconut:</p>

          <div class="img-row">
            <img src="results/mycat2.png" alt="My Cat Coconut" style="max-width: 90%;" />
          </div>

<p><b>Observations:</b> The sharpened images show clearer edges and textures. Increasing α enhances sharpness but too large a value may introduce visible artifacts or unnatural contrast.</p>


          
<h3>2.2 Hybrid Images</h3>
<p>The idea of hybrid images is to combine the low-frequency (blurred) part of one image with the high-frequency (edge/detail) part of another. At close viewing distance, the high frequencies dominate perception, so we see image A. From far away, the high frequencies fade, and the low frequencies dominate, so we see image B.</p>

<h4>Example 1: Derek + Nutmeg</h4>
<p>
For this hybrid, I show the original and aligned inputs, their Fourier transforms, the filtered results, the chosen cutoff frequency (σ = 4), and the final hybrid.  
The cutoff was chosen to preserve Derek’s smooth face while emphasizing Nutmeg’s high-frequency details.
</p>

<div class="img-row">
  <img src="results/nutmeg.jpg" alt="Derek + Nutmeg full process" style="max-width: 95%;" />
</div>

<hr>

<h4>Example 2 & 3: Motorcycle + Bicycle & Strawberry + fish</h4>
<p>Process: keep high frequencies of motorcycle/strawberry, low frequencies of bicycle/fish, and combine them into a hybrid.</p>

<div class="img-row">
  <img src="results/motorcycle.png" alt="Hybrid bicycle and motorcycle" style="max-width: 90%;" />
  <img src="results/motorcycle2.png" alt="Hybrid bicycle and motorcycle" style="max-width: 90%;" />
</div>

<div class="img-row">
  <img src="results/berryfish.png" alt="Hybrid red fish with strawberry" style="max-width: 90%;" />
  <img src="results/berryfish2.png" alt="Hybrid red fish with strawberry" style="max-width: 90%;" />
</div>

<h4>Example 3: Other Hybrids</h4>
<p>I also created multiple hybrid examples (e.g., change of expression, my cat growing older (eating a lot)).</p>

<div class="img-row">
  <img src="results/smile.png" alt="Hybrid expressions" style="max-width: 90%;" />
  <img src="results/coconut.png" alt="Hybrid images when Coconut was young and now" style="max-width: 90%;" />
</div>
        </section>

        <!-- Part 2.3 / 2.4 -->
        <section id="multires" class="card">
          <h2>Part 2.3/2.4: Gaussian &amp; Laplacian Stacks + Multiresolution Blending</h2>
          <h3>Gaussian &amp; Laplacian Stacks (no downsampling)</h3>
          <p>
            A <strong>stack</strong> is similar to a pyramid but every level has the same spatial size as the original image; only the amount of Gaussian smoothing increases at each level. To build a Gaussian stack, repeatedly blur the image (increasing the effective sigma each level) and store each blurred image. The Laplacian stack is formed by taking the difference between successive Gaussian levels (L[i] = G[i] - G[i+1]) with the last level equal to G[N].
          </p>

          <details>
            <summary>Core pseudo-code</summary>
            <pre><code># Gaussian stack
G[0] = img
for i in 1..N:
    G[i] = gaussian_filter(G[i-1], sigma_i)

# Laplacian stack
L[i] = G[i] - G[i+1]
L[N] = G[N]

# Blending with mask
for each level:
   blended = mask_gauss[level] * L1[level] + (1-mask_gauss[level]) * L2[level]
Reconstruct by summing blended levels
</code></pre>
          </details>

          <h3>Multiresolution Blending (the Oraple)</h3>
          <p>
          In this approach, each level of the stack maintains the original image size, with increasing Gaussian smoothing at higher levels. The Laplacian stack is obtained by taking differences between successive Gaussian levels, with the final level equal to the most blurred Gaussian image. For blending, the Laplacian bands from two images are combined at each level using a Gaussian-smoothed mask, and the final blended image is reconstructed by summing all levels from coarse to fine.
          </p>

          <div class="img-row">
            <img src="results/orange.png" alt="apple/orange inputs & oraple result(placeholder)" style="max-width:90%" />
          </div>

          <p>Below are example results: the input pair, the blended result, and a visualization of Laplacian bands for the chosen levels.</p>

          <div class="img-row">
            <figure>
              <img src="results/flowerman.jpg" alt="blending flowers with dresses" style="max-width:90%" />
              <figcaption>Horizontal seam blending: flower head smoothly merged with a dress.</figcaption>
            </figure>
            <figure>
              <img src="results/starhill.png" alt="Blending van Gogh's paintings with hills" style="max-width:90%" />
              <figcaption>Diagonal seam blending: van Gogh's starry sky gradually transitions into hills.</figcaption>
            </figure>
            <figure>
              <img src="results/eyeflower.png" alt="Blending sunflower with an eye" style="max-width:90%" />
              <figcaption>Irregular (circular) mask blending: an eye inside the sunflower center.</figcaption>
            </figure>
          </div>

        </section>

      </main>
    </div>
</body>
</html>

